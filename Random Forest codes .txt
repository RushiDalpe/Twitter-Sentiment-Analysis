Random Forest with 200000 samples 

 
!pip install pandas scikit-learn nltk matplotlib seaborn wordcloud 
import pandas as pd 
import re 
import string 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, 
roc_curve, auc, f1_score, precision_score, recall_score, accuracy_score 
import matplotlib.pyplot as plt 
import seaborn as sns 
from wordcloud import WordCloud 
from collections import Counter 
from google.colab import files 

def clean_text(text): 
text = text.lower() 
text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
text = re.sub(r'\@\w+|\#', '', text) 
text = text.translate(str.maketrans('', '', string.punctuation)) 
text = re.sub(r'\d+', '', text) 
text = re.sub(r'\s+', ' ', text).strip() 
return text 
 
uploaded = files.upload() 
 
file_name = list(uploaded.keys())[0] 
data_sample = pd.read_csv(file_name, encoding='latin-1', usecols=[0, 5], names=['sentiment', 'text'], 
header=None) 
data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0) 
data_sample = data_sample.sample(n=200000, random_state=42) 
data_sample['text'] = data_sample['text'].apply(clean_text) 
train_texts, test_texts, train_labels, test_labels = train_test_split( 
data_sample['text'], data_sample['sentiment'], test_size=0.25, random_state=42, 
stratify=data_sample['sentiment'] 
) 
 
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2)) 
train_vectors = vectorizer.fit_transform(train_texts) 
test_vectors = vectorizer.transform(test_texts) 
model = RandomForestClassifier(n_estimators=100, random_state=42) 
model.fit(train_vectors, train_labels)  
y_pred = model.predict(test_vectors) 
y_probs = model.predict_proba(test_vectors)[:, 1]  
accuracy = accuracy_score(test_labels, y_pred) 
precision = precision_score(test_labels, y_pred) 
recall = recall_score(test_labels, y_pred) 
f1 = f1_score(test_labels, y_pred) 
 
print("Classification Report:") 
print(classification_report(test_labels, y_pred)) 
print(f"Accuracy: {accuracy:.4f}") 
print(f"Precision: {precision:.4f}") 
print(f"Recall: {recall:.4f}") 
print(f"F1 Score: {f1:.4f}") 
cm = confusion_matrix(test_labels, y_pred, labels=[0, 1]) 
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]) 
disp.plot() 
plt.show() 
fpr, tpr, _ = roc_curve(test_labels, y_probs) 
roc_auc = auc(fpr, tpr) 
 
plt.figure() 
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc) 
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') 
plt.xlim([0.0, 1.0]) 
plt.ylim([0.0, 1.05]) 
plt.xlabel('False Positive Rate') 
plt.ylabel('True Positive Rate') 
plt.title('Receiver Operating Characteristic') 
plt.legend(loc='lower right') 
plt.show() 
 
plt.figure(figsize=(8, 6)) 
sns.countplot(x='sentiment', data=data_sample) 
plt.title('Class Distribution') 
plt.xlabel('Sentiment') 
plt.ylabel('Count') 
plt.show() 
 
positive_texts = ' '.join(train_texts[train_labels == 1]) 
negative_texts = ' '.join(train_texts[train_labels == 0]) 
 
positive_wordcloud = WordCloud(width=800, height=400, 
background_color='white').generate(positive_texts) 
plt.figure(figsize=(10, 7)) 
plt.imshow(positive_wordcloud, interpolation="bilinear") 
plt.axis('off') 
plt.title('Most Common Positive Words') 
plt.show() 
positive_word_freq = Counter(positive_texts.split()) 
common_positive_words = positive_word_freq.most_common(20) 
plt.figure(figsize=(12, 8)) 
sns.barplot(x=[word[1] for word in common_positive_words], y=[word[0] for word in 
common_positive_words]) 
plt.title('Top 20 Most Common Positive Words') 
plt.xlabel('Frequency') 
plt.ylabel('Words') 
plt.show() 

negative_wordcloud = WordCloud(width=800, height=400, 
background_color='white').generate(negative_texts) 
plt.figure(figsize=(10, 7)) 
plt.imshow(negative_wordcloud, interpolation="bilinear") 
plt.axis('off') 
plt.title('Most Common Negative Words') 
plt.show() 
negative_word_freq = Counter(negative_texts.split()) 
common_negative_words = negative_word_freq.most_common(20) 
plt.figure(figsize=(12, 8)) 
sns.barplot(x=[word[1] for word in common_negative_words], y=[word[0] for word in 
common_negative_words]) 
plt.title('Top 20 Most Common Negative Words') 
plt.xlabel('Frequency') 
plt.ylabel('Words') 
plt.show() 
 

new_texts = ["I love this!", "This is terrible..."] 
new_vectors = vectorizer.transform(new_texts) 
new_predictions = model.predict(new_vectors) 
new_probabilities = model.predict_proba(new_vectors) 
 
for i, text in enumerate(new_texts): 
    print(f"Text: {text}") 
    print(f"Predicted Sentiment: {'Positive' if new_predictions[i] == 1 else 'Negative'}") 
    print(f"Prediction Confidence: {new_probabilities[i][new_predictions[i]]:.2f}\n")