(1)Logistic regression with training size of 100,000 tweets. 


import pandas as pd 
import re 
import string 
import numpy as np 
from tkinter import Tk 
from tkinter.filedialog import askopenfilename 
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.linear_model import LogisticRegression 
from sklearn.pipeline import Pipeline 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, 
ConfusionMatrixDisplay 
from sklearn.utils.class_weight import compute_class_weight 
import matplotlib.pyplot as plt 
import seaborn as sns 
from wordcloud import WordCloud 
def clean_text(text): 
text = text.lower() 
text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
text = re.sub(r'\@\w+|\#', '', text) 
text = text.translate(str.maketrans('', '', string.punctuation)) 
text = re.sub(r'\d+', '', text) 
text = re.sub(r'\s+', ' ', text).strip() 
return text 
Tk().withdraw() 
file_path = askopenfilename(title="Select the CSV file", filetypes=[("CSV files", "*.csv")]) 
 
if not file_path: 
    print("No file selected.") 
else: 
    try: 
        print("Loading data...") 
        data_sample = pd.read_csv(file_path, encoding='latin-1') 
 
        data_sample = data_sample.iloc[:, [0, 5]] 
        data_sample.columns = ['sentiment', 'text'] 
         
        print("Handling missing values...") 
        data_sample.dropna(inplace=True) 
 
        data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0) 
        data_sample['text'] = data_sample['text'].apply(clean_text) 
 
        class_distribution = data_sample['sentiment'].value_counts() 
        print(f"Class distribution:\n{class_distribution}") 
 
        plt.figure(figsize=(6, 6)) 
        sns.countplot(x='sentiment', data=data_sample) 
        plt.title('Sentiment Distribution') 
        plt.show() 
 
        if len(class_distribution) < 2: 
            raise ValueError("Insufficient samples of one class. Ensure your dataset includes both positive 
and negative samples.") 
 
        min_class_count = class_distribution.min() 
 
        train_size = 100000 
        test_size = int(train_size * 0.25) 
 
        balanced_data = pd.concat([ 
            data_sample[data_sample['sentiment'] == 0].sample(train_size // 2, random_state=42), 
            data_sample[data_sample['sentiment'] == 1].sample(train_size // 2, random_state=42) 
        ]) 
 
        print("Splitting data...") 
        X = balanced_data['text'] 
        y = balanced_data['sentiment'] 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, 
stratify=y) 
 
        print(f"Training set size: {len(X_train)}") 
        print(f"Testing set size: {len(X_test)}") 
 
        all_words = ' '.join([text for text in X_train]) 
        wordcloud = WordCloud(width=800, height=400, random_state=21, 
max_font_size=110).generate(all_words) 
        plt.figure(figsize=(10, 7)) 
        plt.imshow(wordcloud, interpolation="bilinear") 
        plt.axis('off') 
        plt.title('Most Common Words') 
        plt.show() 
 
        data_sample['text_length'] = data_sample['text'].apply(len) 
        plt.figure(figsize=(10, 6)) 
        sns.histplot(data_sample['text_length'], bins=30, kde=True) 
        plt.title('Tweet Length Distribution') 
        plt.show() 
 
        class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), 
y=y_train) 
        class_weights_dict = {0: class_weights[0], 1: class_weights[1]} 
 
        print("Setting up the pipeline and grid search...") 
        pipeline = Pipeline([ 
            ('tfidf', TfidfVectorizer()), 
            ('logreg', LogisticRegression(class_weight=class_weights_dict, max_iter=1000)) 
        ]) 
 
        param_grid = { 
            'tfidf__max_features': [5000, 10000], 
            'tfidf__ngram_range': [(1, 1), (1, 2)], 
            'logreg__C': [0.1, 1, 10] 
        } 
 
        grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2) 
 
        print("Performing grid search...") 
        grid_search.fit(X_train, y_train) 
        print("Grid search completed.") 
 
        best_params = grid_search.best_params_ 
        best_model = grid_search.best_estimator_ 
 
        print(f"Best parameters: {best_params}") 
 
        print("Predicting...") 
        y_pred_logreg = best_model.predict(X_test) 
        print("Prediction completed.") 
 
        print("Evaluating the model...") 
        accuracy_logreg = accuracy_score(y_test, y_pred_logreg) 
        report_logreg = classification_report(y_test, y_pred_logreg) 
 
        print(f"Accuracy: {accuracy_logreg}") 
        print(f"Classification Report:\n{report_logreg}") 
 
        cm = confusion_matrix(y_test, y_pred_logreg, labels=[0, 1]) 
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]) 
        disp.plot() 
        plt.show() 
 
    except Exception as e: 
        print(f"An error occurred: {e}")



(2)Logistic Regression with training size of 500,000 tweets.
 
import pandas as pd 
import re 
import string 
import numpy as np 
from tkinter import Tk 
from tkinter.filedialog import askopenfilename 
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.linear_model import LogisticRegression 
from sklearn.pipeline import Pipeline 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, 
ConfusionMatrixDisplay 
from sklearn.utils.class_weight import compute_class_weight 
import matplotlib.pyplot as plt 
import seaborn as sns 
from wordcloud import WordCloud 
 
def clean_text(text): 
    text = text.lower() 
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
    text = re.sub(r'\@\w+|\#', '', text) 
    text = text.translate(str.maketrans('', '', string.punctuation)) 
    text = re.sub(r'\d+', '', text) 
    text = re.sub(r'\s+', ' ', text).strip() 
    return text 
 
Tk().withdraw() 
  
file_path = askopenfilename(title="Select the CSV file", filetypes=[("CSV files", "*.csv")]) 
 
if not file_path: 
    print("No file selected.") 
else: 
    try: 
        print("Loading data...") 
        data_sample = pd.read_csv(file_path, encoding='latin-1') 
 
        data_sample = data_sample.iloc[:, [0, 5]] 
        data_sample.columns = ['sentiment', 'text'] 
         
        print("Handling missing values...") 
        data_sample.dropna(inplace=True) 
 
        data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0) 
        data_sample['text'] = data_sample['text'].apply(clean_text) 
 
        class_distribution = data_sample['sentiment'].value_counts() 
        print(f"Class distribution:\n{class_distribution}") 
  
        plt.figure(figsize=(6, 6)) 
        sns.countplot(x='sentiment', data=data_sample) 
        plt.title('Sentiment Distribution') 
        plt.show() 
 
        if len(class_distribution) < 2: 
            raise ValueError("Insufficient samples of one class. Ensure your dataset includes both 
positive and negative samples.") 
 
        min_class_count = class_distribution.min() 
 
        train_size = 500000 
        test_size = int(train_size * 0.25) 
 
        balanced_data = pd.concat([ 
            data_sample[data_sample['sentiment'] == 0].sample(train_size // 2, 
random_state=42), 
            data_sample[data_sample['sentiment'] == 1].sample(train_size // 2, 
random_state=42) 
        ]) 
 
        print("Splitting data...") 
        X = balanced_data['text'] 
        y = balanced_data['sentiment'] 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, 
random_state=42, stratify=y) 
 
        print(f"Training set size: {len(X_train)}") 
        print(f"Testing set size: {len(X_test)}") 
  
        all_words = ' '.join([text for text in X_train]) 
        wordcloud = WordCloud(width=800, height=400, random_state=21, 
max_font_size=110).generate(all_words) 
        plt.figure(figsize=(10, 7)) 
        plt.imshow(wordcloud, interpolation="bilinear") 
        plt.axis('off') 
        plt.title('Most Common Words') 
        plt.show() 
  
        data_sample['text_length'] = data_sample['text'].apply(len) 
        plt.figure(figsize=(10, 6)) 
        sns.histplot(data_sample['text_length'], bins=30, kde=True) 
        plt.title('Tweet Length Distribution') 
        plt.show() 
 
        class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 
1]), y=y_train) 
        class_weights_dict = {0: class_weights[0], 1: class_weights[1]} 
 
        print("Setting up the pipeline and grid search...") 
        pipeline = Pipeline([ 
            ('tfidf', TfidfVectorizer()), 
            ('logreg', LogisticRegression(class_weight=class_weights_dict, max_iter=1000)) 
        ]) 
 
        param_grid = { 
            'tfidf__max_features': [5000, 10000], 
            'tfidf__ngram_range': [(1, 1), (1, 2)], 
            'logreg__C': [0.1, 1, 10] 
        } 
 
        grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2) 
 
        print("Performing grid search...") 
        grid_search.fit(X_train, y_train) 
        print("Grid search completed.") 
 
        best_params = grid_search.best_params_ 
        best_model = grid_search.best_estimator_ 
 
        print(f"Best parameters: {best_params}") 
 
        print("Predicting...") 
        y_pred_logreg = best_model.predict(X_test) 
        print("Prediction completed.") 
 
        print("Evaluating the model...") 
        accuracy_logreg = accuracy_score(y_test, y_pred_logreg) 
        report_logreg = classification_report(y_test, y_pred_logreg) 
 
        print(f"Accuracy: {accuracy_logreg}") 
        print(f"Classification Report:\n{report_logreg}") 
 
        cm = confusion_matrix(y_test, y_pred_logreg, labels=[0, 1]) 
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]) 
        disp.plot() 
        plt.show() 
 
    except Exception as e: 
        print(f"An error occurred: {e}")


(3) Logistic regression with full dataset and charts. 

import pandas as pd 
import re 
import string 
import numpy as np 
from nltk.corpus import stopwords 
from nltk.stem import PorterStemmer 
from tkinter import Tk 
from tkinter.filedialog import askopenfilename 
from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.linear_model import LogisticRegression 
from sklearn.pipeline import Pipeline 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, 
ConfusionMatrixDisplay 
from sklearn.utils.class_weight import compute_class_weight 
import matplotlib.pyplot as plt 
import seaborn as sns 
from wordcloud import WordCloud 
 
def clean_text(text): 
text = text.lower() 
text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
text = re.sub(r'\@\w+|\#', '', text)  # Remove mentions and hashtags 
text = text.translate(str.maketrans('', '', string.punctuation))  
text = re.sub(r'\d+', '', text)   
text = re.sub(r'\s+', ' ', text).strip()  
return text 
  
Tk().withdraw() 
  
file_path = askopenfilename(title="Select the CSV file", filetypes=[("CSV files", "*.csv")]) 
 
if not file_path: 
    print("No file selected.") 
else: 
    try: 
        
        print("Loading data...") 
        data_sample = pd.read_csv(file_path, encoding='latin-1') 
 
        data_sample = data_sample.iloc[:, [0, 5]] 
        data_sample.columns = ['sentiment', 'text'] 
 
        print("Handling missing values...") 
        data_sample.dropna(inplace=True) 
 
        data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0) 
 
        data_sample['text'] = data_sample['text'].apply(clean_text) 
  
        class_distribution = data_sample['sentiment'].value_counts() 
        print(f"Class distribution:\n{class_distribution}") 
 
        if len(class_distribution) < 2: 
            raise ValueError("Insufficient samples of one class. Ensure your dataset includes both 
positive and negative samples.") 
  
        min_class_count = class_distribution.min() 
 
        train_size = 1279998 
        test_size = int(train_size * 0.25)  # 25% of train_size for test 
         
        balanced_data = pd.concat([ 
            data_sample[data_sample['sentiment'] == 0].sample(train_size // 2, 
random_state=42), 
            data_sample[data_sample['sentiment'] == 1].sample(train_size // 2, 
random_state=42) 
        ]) 
 
        print("Splitting data...") 
        X = balanced_data['text'] 
        y = balanced_data['sentiment'] 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, 
random_state=42, stratify=y) 
  
        print(f"Training set size: {len(X_train)}") 
        print(f"Testing set size: {len(X_test)}") 
 
        all_words = ' '.join([text for text in X_train]) 
        wordcloud = WordCloud(width=800, height=400, random_state=21, 
max_font_size=110).generate(all_words) 
        plt.figure(figsize=(10, 7)) 
        plt.imshow(wordcloud, interpolation="bilinear") 
        plt.axis('off') 
        plt.title('Most Common Words') 
        plt.show() 
 
        data_sample['text_length'] = data_sample['text'].apply(len) 
        plt.figure(figsize=(10, 6)) 
        sns.histplot(data_sample['text_length'], bins=30, kde=True) 
        plt.title('Tweet Length Distribution') 
        plt.show() 
 
        class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 
1]), y=y_train) 
        class_weights_dict = {0: class_weights[0], 1: class_weights[1]} 
  
        print("Setting up the pipeline and grid search...") 
        pipeline = Pipeline([ 
            ('tfidf', TfidfVectorizer()), 
            ('logreg', LogisticRegression(class_weight=class_weights_dict, max_iter=1000)) 
        ]) 
 
        param_grid = { 
            'tfidf__max_features': [5000, 10000], 
            'tfidf__ngram_range': [(1, 1), (1, 2)], 
            'logreg__C': [0.1, 1, 10] 
        } 
 
        grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2) 
 
        print("Performing grid search...") 
        grid_search.fit(X_train, y_train) 
        print("Grid search completed.") 
 
        best_params = grid_search.best_params_ 
        best_model = grid_search.best_estimator_ 
 
        print(f"Best parameters: {best_params}") 
 
        print("Predicting...") 
        y_pred_logreg = best_model.predict(X_test) 
        print("Prediction completed.") 

        print("Evaluating the model...") 
        accuracy_logreg = accuracy_score(y_test, y_pred_logreg) 
        report_logreg = classification_report(y_test, y_pred_logreg) 
 
        print(f"Accuracy: {accuracy_logreg}") 
        print(f"Classification Report:\n{report_logreg}") 
 
        cm = confusion_matrix(y_test, y_pred_logreg, labels=[0, 1]) 
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]) 
        disp.plot() 
        plt.show() 
 
        feature_names = best_model.named_steps['tfidf'].get_feature_names_out() 
        top_n = 20 
        coefs = best_model.named_steps['logreg'].coef_.flatten() 
        top_positive_coefficients = np.argsort(coefs)[-top_n:] 
        top_negative_coefficients = np.argsort(coefs)[:top_n] 
        top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients]) 
 
        plt.figure(figsize=(15, 5)) 
        colors = ['red' if c < 0 else 'blue' for c in coefs[top_coefficients]] 
        plt.bar(np.arange(2 * top_n), coefs[top_coefficients], color=colors) 
        feature_names = np.array(feature_names) 
        plt.xticks(np.arange(2 * top_n), feature_names[top_coefficients], rotation=60, 
ha='right') 
        plt.title('Top TF-IDF Features') 
        plt.show() 
 
    except Exception as e: 
print(f"An error occurred: {e}")