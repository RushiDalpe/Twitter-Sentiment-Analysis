DistilBERT code

!pip install transformers pandas scikit-learn torch nltk matplotlib seaborn

import pandas as pd
import re
import string
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from transformers import TextClassificationPipeline
from torch.utils.data import DataLoader, Dataset
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from google.colab import files

def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

uploaded = files.upload()

file_name = list(uploaded.keys())[0]
data_sample = pd.read_csv(file_name, encoding='latin-1', usecols=[0, 5], names=['sentiment', 'text'], header=None)
data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0)

data_sample = data_sample.sample(n=50000, random_state=42)

data_sample['text'] = data_sample['text'].apply(clean_text)

train_texts, test_texts, train_labels, test_labels = train_test_split(
    data_sample['text'], data_sample['sentiment'], test_size=0.25, random_state=42, stratify=data_sample['sentiment']
)

print(f"Number of training samples: {len(train_texts)}")
print(f"Number of testing samples: {len(test_texts)}")

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512)

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

train_dataset = SentimentDataset(train_encodings, train_labels.tolist())
test_dataset = SentimentDataset(test_encodings, test_labels.tolist())

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

from torch.optim import AdamW

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="steps",
    save_total_limit=1,
    save_steps=500,
    eval_steps=500,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  
)

train_result = trainer.train()
trainer.save_model()

eval_result = trainer.evaluate()

nlp = TextClassificationPipeline(model=model, tokenizer=tokenizer)
preds = nlp(list(test_texts))

y_true = test_labels
y_pred = [int(pred['label'].split('_')[-1]) for pred in preds]
y_probs = [pred['score'] if pred['label'] == 'LABEL_1' else 1 - pred['score'] for pred in preds]

print(classification_report(y_true, y_pred))
cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
disp.plot()
plt.show()

train_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log]
eval_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]

plt.figure(figsize=(8, 6))
plt.plot(train_loss, label='Train')
plt.plot(eval_loss, label='Test')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Step')
plt.legend(loc='upper left')
plt.show()

fpr, tpr, _ = roc_curve(y_true, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x='sentiment', data=data_sample)
plt.title('Class Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()
