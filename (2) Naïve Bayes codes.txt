(2) Naïve Bayes with 100000 training size with charts. 

import pandas as pd 
import re 
import string 
import numpy as np 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.pipeline import Pipeline 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, 
import matplotlib.pyplot as plt 
from tkinter import Tk 
from tkinter.filedialog import askopenfilename 
from wordcloud import WordCloud 
import seaborn as sns 
def clean_text(text): 
text = text.lower() 
text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
text = re.sub(r'\@\w+|\#', '', text) 
text = text.translate(str.maketrans('', '', string.punctuation)) 
text = re.sub(r'\d+', '', text) 
text = re.sub(r'\s+', ' ', text).strip() 
return text 
def select_file(): 
root = Tk() 
root.withdraw() 
    file_path = askopenfilename(title="Select the CSV file", filetypes=[("CSV files", "*.csv")]) 
    root.update() 
    root.destroy()  
    return file_path 
 
file_path = select_file() 
 
if not file_path: 
    print("No file selected.") 
else: 
    try: 
        data_sample = pd.read_csv(file_path, encoding='latin-1') 
        data_sample = data_sample.iloc[:, [0, 5]] 
        data_sample.columns = ['sentiment', 'text'] 
        data_sample.dropna(inplace=True) 
        data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0) 
        data_sample['text'] = data_sample['text'].apply(clean_text) 
 
        train_size = 100000 
        print(f"\nTraining with sample size: {train_size}") 
        balanced_data = pd.concat([ 
            data_sample[data_sample['sentiment'] == 0].sample(train_size // 2, 
random_state=42), 
            data_sample[data_sample['sentiment'] == 1].sample(train_size // 2, 
random_state=42) 
        ]) 
        X = balanced_data['text'] 
        y = balanced_data['sentiment'] 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, 
stratify=y) 
 
        pipeline = Pipeline([ 
            ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))), 
            ('nb', MultinomialNB()) 
        ]) 
 
        pipeline.fit(X_train, y_train) 
 
        y_pred_nb = pipeline.predict(X_test) 
        accuracy_nb = accuracy_score(y_test, y_pred_nb) 
        report_nb = classification_report(y_test, y_pred_nb) 
 
        print(f"Accuracy: {accuracy_nb}") 
        print(f"Classification Report:\n{report_nb}") 
 
        cm = confusion_matrix(y_test, y_pred_nb, labels=[0, 1]) 
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]) 
        disp.plot() 
        plt.show() 
 
        all_words = ' '.join([text for text in X_train]) 
        wordcloud = WordCloud(width=800, height=400, random_state=21, 
max_font_size=110).generate(all_words) 
        plt.figure(figsize=(10, 7)) 
        plt.imshow(wordcloud, interpolation="bilinear") 
        plt.axis('off') 
        plt.title('Most Common Words') 
        plt.show() 
 
        data_sample['text_length'] = data_sample['text'].apply(len) 
        plt.figure(figsize=(10, 6)) 
        sns.histplot(data_sample['text_length'], bins=30, kde=True) 
        plt.title('Tweet Length Distribution') 
        plt.show() 
 
        tfidf = pipeline.named_steps['tfidf'] 
        nb = pipeline.named_steps['nb'] 
        feature_names = tfidf.get_feature_names_out() 
        top_n = 20 
        log_prob = nb.feature_log_prob_ 
        top_features = np.argsort(log_prob[1])[-top_n:] 
        top_features_names = [feature_names[i] for i in top_features] 
 
        plt.figure(figsize=(15, 5)) 
        plt.barh(range(top_n), log_prob[1][top_features], align='center', color='blue') 
        plt.yticks(np.arange(top_n), top_features_names) 
        plt.xlabel('Log Probability') 
        plt.title('Top TF-IDF Features') 
        plt.show() 
 
    except Exception as e: 
        print(f"An error occurred: {e}")


(2) Naïve Bayes with 500,000 training set with charts.

import pandas as pd 
import re 
import string 
import numpy as np 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.pipeline import Pipeline 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  
import matplotlib.pyplot as plt 
from tkinter import Tk 
from tkinter.filedialog import askopenfilename 
from wordcloud import WordCloud 
import seaborn as sns 
 
def clean_text(text): 
    text = text.lower() 
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
    text = re.sub(r'\@\w+|\#', '', text) 
    text = text.translate(str.maketrans('', '', string.punctuation)) 
    text = re.sub(r'\d+', '', text) 
    text = re.sub(r'\s+', ' ', text).strip() 
    return text 
 
def select_file(): 
    root = Tk() 
    root.withdraw()  
    file_path = askopenfilename(title="Select the CSV file", filetypes=[("CSV files", "*.csv")]) 
    root.update()   
    root.destroy()   
    return file_path 
 
file_path = select_file() 
 
if not file_path: 
    print("No file selected.") 
else: 
    try: 
        data_sample = pd.read_csv(file_path, encoding='latin-1') 
        data_sample = data_sample.iloc[:, [0, 5]] 
        data_sample.columns = ['sentiment', 'text'] 
        data_sample.dropna(inplace=True) 
        data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0) 
        data_sample['text'] = data_sample['text'].apply(clean_text) 
 
        train_size = 500000 
        print(f"\nTraining with sample size: {train_size}") 
        balanced_data = pd.concat([ 
            data_sample[data_sample['sentiment'] == 0].sample(train_size // 2, 
random_state=42), 
            data_sample[data_sample['sentiment'] == 1].sample(train_size // 2, 
random_state=42) 
        ]) 
        X = balanced_data['text'] 
        y = balanced_data['sentiment'] 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, 
stratify=y) 
 
        pipeline = Pipeline([ 
            ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))), 
            ('nb', MultinomialNB()) 
        ]) 
 
        pipeline.fit(X_train, y_train) 
 
        y_pred_nb = pipeline.predict(X_test) 
        accuracy_nb = accuracy_score(y_test, y_pred_nb) 
        report_nb = classification_report(y_test, y_pred_nb) 
 
        print(f"Accuracy: {accuracy_nb}") 
        print(f"Classification Report:\n{report_nb}") 
  
        cm = confusion_matrix(y_test, y_pred_nb, labels=[0, 1]) 
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]) 
        disp.plot() 
        plt.show() 
  
        all_words = ' '.join([text for text in X_train]) 
        wordcloud = WordCloud(width=800, height=400, random_state=21, 
max_font_size=110).generate(all_words) 
        plt.figure(figsize=(10, 7)) 
        plt.imshow(wordcloud, interpolation="bilinear") 
        plt.axis('off') 
        plt.title('Most Common Words') 
        plt.show() 
  
        data_sample['text_length'] = data_sample['text'].apply(len) 
        plt.figure(figsize=(10, 6)) 
        sns.histplot(data_sample['text_length'], bins=30, kde=True) 
        plt.title('Tweet Length Distribution') 
        plt.show() 
  
        tfidf = pipeline.named_steps['tfidf'] 
        nb = pipeline.named_steps['nb'] 
        feature_names = tfidf.get_feature_names_out() 
        top_n = 20 
        log_prob = nb.feature_log_prob_ 
        top_features = np.argsort(log_prob[1])[-top_n:] 
        top_features_names = [feature_names[i] for i in top_features] 
 
        plt.figure(figsize=(15, 5)) 
        plt.barh(range(top_n), log_prob[1][top_features], align='center', color='blue') 
        plt.yticks(np.arange(top_n), top_features_names) 
        plt.xlabel('Log Probability') 
        plt.title('Top TF-IDF Features') 
        plt.show() 
 
    except Exception as e: 
        print(f"An error occurred: {e}")


(3) Naïve Baye with full dataset with charts. 

import pandas as pd 
import re 
import string 
import numpy as np 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.pipeline import Pipeline 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  
import matplotlib.pyplot as plt 
from tkinter import Tk 
from tkinter.filedialog import askopenfilename 
from wordcloud import WordCloud 
import seaborn as sns 
def clean_text(text): 
text = text.lower() 
text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) 
text = re.sub(r'\@\w+|\#', '', text) 
text = text.translate(str.maketrans('', '', string.punctuation)) 
text = re.sub(r'\d+', '', text) 
text = re.sub(r'\s+', ' ', text).strip() 
return text 
def select_file(): 
root = Tk() 
root.withdraw()  
    file_path = askopenfilename(title="Select the CSV file", filetypes=[("CSV files", "*.csv")]) 
    root.update()  
    root.destroy()  
    return file_path 
 
file_path = select_file() 
 
if not file_path: 
    print("No file selected.") 
else: 
    try: 
        data_sample = pd.read_csv(file_path, encoding='latin-1') 
        data_sample = data_sample.iloc[:, [0, 5]] 
        data_sample.columns = ['sentiment', 'text'] 
        data_sample.dropna(inplace=True) 
        data_sample['sentiment'] = data_sample['sentiment'].apply(lambda x: 1 if x == 4 else 0) 
        data_sample['text'] = data_sample['text'].apply(clean_text) 
 
        train_size = 1279998 
        print(f"\nTraining with sample size: {train_size}") 
        balanced_data = pd.concat([ 
            data_sample[data_sample['sentiment'] == 0].sample(train_size // 2, 
random_state=42), 
            data_sample[data_sample['sentiment'] == 1].sample(train_size // 2, 
random_state=42) 
        ]) 
        X = balanced_data['text'] 
        y = balanced_data['sentiment'] 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, 
stratify=y) 
 
        pipeline = Pipeline([ 
            ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))), 
            ('nb', MultinomialNB()) 
        ]) 
 
        pipeline.fit(X_train, y_train) 
 
        y_pred_nb = pipeline.predict(X_test) 
        accuracy_nb = accuracy_score(y_test, y_pred_nb) 
        report_nb = classification_report(y_test, y_pred_nb) 
 
        print(f"Accuracy: {accuracy_nb}") 
        print(f"Classification Report:\n{report_nb}") 
 
        cm = confusion_matrix(y_test, y_pred_nb, labels=[0, 1]) 
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]) 
        disp.plot() 
        plt.show() 
  
        all_words = ' '.join([text for text in X_train]) 
        wordcloud = WordCloud(width=800, height=400, random_state=21, 
max_font_size=110).generate(all_words) 
        plt.figure(figsize=(10, 7)) 
        plt.imshow(wordcloud, interpolation="bilinear") 
        plt.axis('off') 
        plt.title('Most Common Words') 
        plt.show() 
  
        data_sample['text_length'] = data_sample['text'].apply(len) 
        plt.figure(figsize=(10, 6)) 
        sns.histplot(data_sample['text_length'], bins=30, kde=True) 
        plt.title('Tweet Length Distribution') 
        plt.show() 
  
        tfidf = pipeline.named_steps['tfidf'] 
        nb = pipeline.named_steps['nb'] 
        feature_names = tfidf.get_feature_names_out() 
        top_n = 20 
        log_prob = nb.feature_log_prob_ 
        top_features = np.argsort(log_prob[1])[-top_n:] 
        top_features_names = [feature_names[i] for i in top_features] 
 
        plt.figure(figsize=(15, 5)) 
        plt.barh(range(top_n), log_prob[1][top_features], align='center', color='blue') 
        plt.yticks(np.arange(top_n), top_features_names) 
        plt.xlabel('Log Probability') 
        plt.title('Top TF-IDF Features') 
        plt.show() 
 
    except Exception as e: 
        print(f"An error occurred: {e}")